What characterizes a **Stationary** random process?;Its statistical distribution is invariant to time shifts.;Its time averages always equal its ensemble averages.;Its mean and autocorrelation vary sinusoidally with time.;It must be a Gaussian process.;;1000
What is the condition for a random process to be **Ergodic**?;Its time averages equal its ensemble averages asymptotically.;It must be strictly stationary and Gaussian.;It implies that the process has zero mean and unit variance.;Its power spectral density is constant.;;1000
According to **Jensen's Inequality**, for a **convex** function \( f(\cdot) \) and a random variable \( x \), which inequality holds?;\( f(\mathbb{E}[x]) \le \mathbb{E}[f(x)] \);\( \mathbb{E}[f(x)] \le f(\mathbb{E}[x]) \);\( f(\mathbb{E}[x]) = \mathbb{E}[f(x)] \);\( f(\mathbb{E}[x]) \ge \mathbb{E}[f(x)]^2 \);;1000
What is the formula for **Discrete Entropy** \( H(x) \)?;\( -\mathbb{E}[\log_{2}(p(x))] \);\( -\sum p(x) \ln(p(x)) \);\( \mathbb{E}[p(x) \log_{2}(p(x))] \);\( \sum p(x)^2 \);;1000
What is the **Joint Entropy** \( H(x_0, x_1) \) if \( x_0 \) and \( x_1 \) are **independent**?;\( H(x_0) + H(x_1) \);\( H(x_0) - H(x_1) \);\( H(x_0) \cdot H(x_1) \);\( 0 \);;1000
Which equation represents the entropy **Chain Rule** for two variables?;\( H(x_0, x_1) = H(x_1) + H(x_0|x_1) \);\( H(x_0, x_1) = H(x_1) - H(x_0|x_1) \);\( H(x_0, x_1) = H(x_1) \cdot H(x_0|x_1) \);\( H(x_0, x_1) = H(x_0) + H(x_1) \);;1000
What is the **Differential Entropy** of a **Complex Gaussian Scalar** \( x \sim \mathcal{N}_{\mathbb{C}}(\mu_x, \sigma_x^2) \)?;\( \log_2(\pi e \sigma_x^2) \);\( \frac{1}{2} \log_2(2 \pi e \sigma_x^2) \);\( \log_2(\pi \sigma_x^2) \);\( \ln(\pi e \sigma_x^2) \);;1000
What is the **Differential Entropy** of a **Complex Gaussian Vector** \( x \sim \mathcal{N}_{\mathbb{C}}(\mu, R) \)?;\( \log_2(\det(\pi e R)) \);\( \log_2(\det(\pi R)) \);\( \det(\log_2(\pi e R)) \);\( n \log_2(\det(R)) \);;1000
How is the **Entropy Rate** \( \mathcal{H} \) defined for a stationary process?;\( \lim_{N\rightarrow\infty} \frac{1}{N} H(x_0, ..., x_{N-1}) \);\( \lim_{N\rightarrow\infty} H(x_N) \);\( \lim_{N\rightarrow\infty} \sum H(x_i) \);\( \frac{1}{N} \sum_{i=0}^{N-1} H(x_i) \);;1000
What is the formula for **Relative Entropy** (Kullback-Leibler Divergence) \( D(p||q) \)?;\( \mathbb{E}[\log_2 \frac{p(x)}{q(x)}] \);\( \mathbb{E}[\log_2 \frac{q(x)}{p(x)}] \);\( -\sum p(x) \log_2 q(x) \);\( \sum p(x) (1 - \frac{q(x)}{p(x)}) \);;1000
How is **Mutual Information** \( \mathcal{I}(s;y) \) defined in terms of entropy?;\( H(s) - H(s|y) \);\( H(s) + H(s|y) \);\( H(y|s) - H(y) \);\( H(s|y) - H(s) \);;1000
What is the **Gaussian Mutual Information** for a Complex Scalar with SNR \( \rho \)?;\( \log_2(1 + \rho) \);\( \log_2(\rho) \);\( \frac{1}{2} \log_2(1 + \rho) \);\( \ln(1 + \rho) \);;1000
What is the approximation of Gaussian Mutual Information for **small** \( \rho \)?;\( \rho \log_2 e \);\( \log_2 \rho \);\( \rho^2 \log_2 e \);\( 1 + \rho \);;1000
What is the approximation of Gaussian Mutual Information for **large** \( \rho \)?;\( \log_2 \rho \);\( \rho \log_2 e \);\( \log_2(1 - \rho) \);\( \rho \);;1000
For a **Complex Gaussian Vector**, what is the Mutual Information \( \mathcal{I}(\rho) \) at low \( \rho \)?;\( \text{tr}(\rho A R_s A^* R_z^{-1} + I) \rho \log_2 e \);\( \log_2(\det(\rho A R_s A^*)) \);\( \text{tr}(R_s) \log_2 \rho \);\( \min(N_s, N_y) \log_2 \rho \);;1000
How is **Channel Capacity** \( C \) defined for a memoryless channel?;\( \max_{\text{constraints}} \mathcal{I}(s; y) \);\( \min_{\text{constraints}} H(s|y) \);\( \max_{\text{constraints}} H(s) \);\( \mathbb{E}[\mathcal{I}(s;y)] \);;1000
In **Maximum Likelihood (ML) Decoding** with Gaussian noise, maximizing the likelihood function is equivalent to?;Minimizing the Euclidean distance \( ||y - \sqrt{\rho} A s_m||^2 \);Maximizing the Euclidean distance \( ||y - \sqrt{\rho} A s_m||^2 \);Minimizing the noise variance;Maximizing the signal power;;1000
What is the general expression for the **MMSE Estimator** \( \hat{s}(y) \)?;\( \mathbb{E}[s|y] \);\( \text{argmax } p(y|s) \);\( \mathbb{E}[y|s] \);\( \mathbb{E}[s] \);;1000
What does the **Orthogonality Principle** state regarding the estimation error \( s - \hat{s}(y) \)?;\( \mathbb{E}[g^*(y)(s-\hat{s}(y))] = 0 \) for any function \( g \);\( \mathbb{E}[(s-\hat{s}(y))^2] = 0 \);\( \mathbb{E}[s(s-\hat{s}(y))^*] = 1 \);The error is orthogonal to the transmitted signal \( s \).;;1000
What is the **MMSE** value for a scalar signal in Gaussian noise with SNR \( \rho \)?;\( \frac{1}{1+\rho} \);\( \frac{\rho}{1+\rho} \);\( 1+\rho \);\( \frac{1}{\rho} \);;1000
What is the **I-MMSE Relationship** for a scalar Gaussian channel?;\( \frac{1}{\log_2 e} \frac{\partial}{\partial \rho} \mathcal{I}(\rho) = \text{MMSE}(\rho) \);\( \frac{\partial}{\partial \rho} \text{MMSE}(\rho) = \mathcal{I}(\rho) \);\( \mathcal{I}(\rho) \cdot \text{MMSE}(\rho) = 1 \);\( \mathcal{I}(\rho) = \text{MMSE}(\rho) \log_2 e \);;1000
What is the formula for the **Linear MMSE (LMMSE)** estimator weight matrix \( W^{MMSE} \)?;\( R_y^{-1} R_{ys} \);\( R_{ys} R_y^{-1} \);\( R_s R_y^{-1} \);\( R_y R_{ys}^{-1} \);;1000
What is the **I-MMSE relationship for vectors** involving the gradient \( \nabla_A \)?;\( \frac{1}{\log_2 e} \nabla_A I(s; \sqrt{\rho}As+z) = \rho A E \);\( \nabla_A I(\rho) = E \);\( \frac{1}{\log_2 e} \nabla_A E = \rho I(\rho) \);\( \nabla_A I = \text{tr}(E) \);;1000